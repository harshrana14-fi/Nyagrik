---
title: "[Part II] – New Performance metrics might be in place in the IPO!"
date: "2025-06-25T09:55:04+00:00"
excerpt: "In the previous post, we had discussed the new OO 537/2025 that is allegedly in place to measure the performance of patent officials. I discussed the metrics in detail in the context of how it incentivises issuing reasoned orders in some instances, while not in other instances. In this post, I will discuss the need…"
image: "/images/part-ii-new-performance-metrics-might-be-in-place-in-the-ipo.png?fit=475%2C476&ssl=1"
ogImage: "https://nyay.legal/images/part-ii-new-performance-metrics-might-be-in-place-in-the-ipo.png?fit=475%2C476&ssl=1"
readingTime: "5 min read"
featured: true
sections:
  - title: "Main"
    content: |
      In the previous post, we had discussed the new OO 537/2025 that is allegedly in place to measure the performance of patent officials. I discussed the metrics in detail in the context of how it incentivises issuing reasoned orders in some instances, while not in other instances. In this post, I will discuss the need for a reasonableness checklist, how the metrics apply to PGO and finally what it means for patent quality.
      
      Questions need to be raised as to when an order becomes unreasoned. Unreasoned orders have been subject to regular discussion in this blog, yet determining whether an order is reasoned is not a straightforward matter. The answer to the question of when an order is unreasoned lies in looking at judicial precedent to figure out when and for what reasons the High Court sets aside an order by a controller. This is not to say that Courts are picture perfect every time they issue a judgment/order. Yet questions of what is a reasoned order are judicially construed concepts, and hence, judgments and judicial opinions are the best sources of their meaning. Hence, the preparation of a reasonableness checklist after a detailed study of judicial decisions can aid in measuring reasonableness in a more concrete sense.
      
      A bare bones checklist can check for the following things:
      
      This checklist can also be updated by borrowing from good practices that Controllers are already following or might come up with in the future and every time the Court sets aside an order for being unreasoned.
      
      Now, coming to pre-grant opposition, it’s again curious that a pre-facie determination and rejection of the opposition (with or without hearing) has been granted the same amount of points as compared to a full-blown examination of pre-grant opposition filed against an application. As per the new rules, the Controller is required to make a prima facie determination and communicate the same to the opponent. It’s intuitive that a prima facie determination will require less time and effort (irrespective of how strict the scrutiny is) than a full-blown examination of the submissions. Yet, unfortunately, the performance metric treats both as equal and grants them the same number of points. This might create an unenviable situation wherein the Controllers have all the incentive to have a very strict scrutiny at the prima facie stage, looking for reasons to reject the representation, because if the prima facie case is made out, then he might have to sink in additional work for the same amount of points he might have gotten had he rejected the application at the prima-facie stage. I have to make it clear, there are other countervailing forces like respective Controllers’ professionalism, reputation and other idiosyncratic factors that might counterbalance this, yet it does not make sense to institutionalise an incentive system that treats prima facie determination and full-blown examination of a PGO as equal.
      
      The previous posts on patent quality (see here and here) had discussed patent quality in the Indian context and argued that hard and inflexible targets do not help, even though defining and measuring patent quality is a nebulous task. New Performance metrics are a step in the right direction. I had argued previously that there were four factors at play and with regard to the new performance metrics, the first two issues have been addressed to an extent.Firstly, the burden on the patent officials could be reduced for the following reason that the new metrics is a more holistic measure and hence a more accurate map relative to Order 34/2016 of the workload that might fall on the shoulders of the Patent Officials. Hence, the patent officials have multiple ways to accure the necessary 100 points on an application, unlike the case under Order 34/2016 where fixed hard targets were provided.
      
      Thus, the first two criticisms I had levied in the previous post, with regard to emphasis on quantity over quality and inadequate incentives for issuing legally robust orders, have been addressed to some extent in the new order by requiring Controllers in some instances, to issue a reasoned order while also incentivising the controller to go for a direct grant which does not require a reasoned order as a requirement (which is problematic). A reasoned order does not need to automatically translate to more patent quality in all contexts, but it is a very important aspect of it. Issuing reasoned orders is a good process that might yield good results (better patent quality). Since there is always the possibility that an order is reasoned, yet the Court on appeal or during an infringement suit might reason otherwise about the validity of the patent.As to the final two criticisms, even under the new performance metrics, the Controller might still engage in strategic behaviour, as the Controller can still net around 6 points for issuing a grant without a reasoned order. Even though this is remedied by the fact that a Controller would get 20 points if the Court upholds his order. A controller can always conclude to behave strategically and issue a grant without a reasoned order in the case of applicants who are more likely to appeal the order of a controller. Yet only empirical data can tell us whether this phenomenon is widespread and whether it requires any form of intervention.
      
      In conclusion, the new performance metrics are a step in the right direction, even though they raise some serious concerns with regards to direct grants and PGO. Would like to hear readers thoughts on this new and interesting development.
      
      I wish to thank Swaraj and Arnav Kaman for their inputs!
---

## Main

In the previous post, we had discussed the new OO 537/2025 that is allegedly in place to measure the performance of patent officials. I discussed the metrics in detail in the context of how it incentivises issuing reasoned orders in some instances, while not in other instances. In this post, I will discuss the need for a reasonableness checklist, how the metrics apply to PGO and finally what it means for patent quality.

Questions need to be raised as to when an order becomes unreasoned. Unreasoned orders have been subject to regular discussion in this blog, yet determining whether an order is reasoned is not a straightforward matter. The answer to the question of when an order is unreasoned lies in looking at judicial precedent to figure out when and for what reasons the High Court sets aside an order by a controller. This is not to say that Courts are picture perfect every time they issue a judgment/order. Yet questions of what is a reasoned order are judicially construed concepts, and hence, judgments and judicial opinions are the best sources of their meaning. Hence, the preparation of a reasonableness checklist after a detailed study of judicial decisions can aid in measuring reasonableness in a more concrete sense.

A bare bones checklist can check for the following things:

This checklist can also be updated by borrowing from good practices that Controllers are already following or might come up with in the future and every time the Court sets aside an order for being unreasoned.

Now, coming to pre-grant opposition, it’s again curious that a pre-facie determination and rejection of the opposition (with or without hearing) has been granted the same amount of points as compared to a full-blown examination of pre-grant opposition filed against an application. As per the new rules, the Controller is required to make a prima facie determination and communicate the same to the opponent. It’s intuitive that a prima facie determination will require less time and effort (irrespective of how strict the scrutiny is) than a full-blown examination of the submissions. Yet, unfortunately, the performance metric treats both as equal and grants them the same number of points. This might create an unenviable situation wherein the Controllers have all the incentive to have a very strict scrutiny at the prima facie stage, looking for reasons to reject the representation, because if the prima facie case is made out, then he might have to sink in additional work for the same amount of points he might have gotten had he rejected the application at the prima-facie stage. I have to make it clear, there are other countervailing forces like respective Controllers’ professionalism, reputation and other idiosyncratic factors that might counterbalance this, yet it does not make sense to institutionalise an incentive system that treats prima facie determination and full-blown examination of a PGO as equal.

The previous posts on patent quality (see here and here) had discussed patent quality in the Indian context and argued that hard and inflexible targets do not help, even though defining and measuring patent quality is a nebulous task. New Performance metrics are a step in the right direction. I had argued previously that there were four factors at play and with regard to the new performance metrics, the first two issues have been addressed to an extent.Firstly, the burden on the patent officials could be reduced for the following reason that the new metrics is a more holistic measure and hence a more accurate map relative to Order 34/2016 of the workload that might fall on the shoulders of the Patent Officials. Hence, the patent officials have multiple ways to accure the necessary 100 points on an application, unlike the case under Order 34/2016 where fixed hard targets were provided.

Thus, the first two criticisms I had levied in the previous post, with regard to emphasis on quantity over quality and inadequate incentives for issuing legally robust orders, have been addressed to some extent in the new order by requiring Controllers in some instances, to issue a reasoned order while also incentivising the controller to go for a direct grant which does not require a reasoned order as a requirement (which is problematic). A reasoned order does not need to automatically translate to more patent quality in all contexts, but it is a very important aspect of it. Issuing reasoned orders is a good process that might yield good results (better patent quality). Since there is always the possibility that an order is reasoned, yet the Court on appeal or during an infringement suit might reason otherwise about the validity of the patent.As to the final two criticisms, even under the new performance metrics, the Controller might still engage in strategic behaviour, as the Controller can still net around 6 points for issuing a grant without a reasoned order. Even though this is remedied by the fact that a Controller would get 20 points if the Court upholds his order. A controller can always conclude to behave strategically and issue a grant without a reasoned order in the case of applicants who are more likely to appeal the order of a controller. Yet only empirical data can tell us whether this phenomenon is widespread and whether it requires any form of intervention.

In conclusion, the new performance metrics are a step in the right direction, even though they raise some serious concerns with regards to direct grants and PGO. Would like to hear readers thoughts on this new and interesting development.

I wish to thank Swaraj and Arnav Kaman for their inputs!